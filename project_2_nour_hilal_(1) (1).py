# -*- coding: utf-8 -*-
"""Project_2_Nour_Hilal (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J7h5PJL9aN5fyvBm10_-L8nzo85n0tCr

# Fundamentals of Deep Learning



Student Name: Nour Hilal

Project 2

# Arabizi text Abusive Language Detection

In this Project we will be constructing a sequence to sequence NLP model which classifies Arabizi text ass offensive or non-Offensive.

## Loading the data and Reading it

We first load the data, which exists in a csv format and divide the text data into a text array and the label data intoa label array.This occurs as follows.
"""

import os
import pandas as pd
import numpy as np
data =pd.read_csv ('/content/Arabizi-Off_Lang_Dataset.csv')
label=data['Generic Class']
text=data['Text']

"""We Check what our text data and label data looks like:"""

print(label)
print(text)

"""We can see that the label data is a string and it is either "offensive" to refer to a text as containing abusive language, or non-offensive to refer to the opposite.We therfore have a binary Classification problem in our hands. It is then better to convert our "offensive" and "non_offensive" label string into 0 and 1.We do that through the following code:"""

label = np.array([1 if lbl == 'offensive' else 0 for lbl in label])

"""## Tokenization of the text data

Right now, our dataset consists of a set of sentences, each made up of a series of words. We want to give our model a way of representing those words in a way that it can understand. With tokenization, we separate a piece of text into smaller chunks (tokens), which in this case are words. Each unique word is then assigned a number, as this is a way that our model can understand the data. Keras has a class that will help us tokenize our data:
"""

from tensorflow.keras.preprocessing.text import Tokenizer

# Tokenize the words in our headlines
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text)

total_words = len(tokenizer.word_index) + 1
print('Total words: ', total_words)

"""## Creating Sequences

Now that we've tokenized the data, turning each word into a representative number, we will create sequences of tokens from the headlines. These sequences are what we will train our deep learning model on.
"""

sequences = tokenizer.texts_to_sequences(text)

"""## Padding Sequences

Right now our sequences are of various lengths. For our model to be able to train on the data, we need to make all the sequences the same length. To do this we'll add padding to the sequences. Keras has a built-in `pad_sequences` [method](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) that we can use.
"""

from tensorflow.keras.preprocessing.sequence import pad_sequences

max_sequence_length = max(len(seq) for seq in sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

"""## Splitting our data into training and testing datasets"""

from sklearn.model_selection import train_test_split

input_sequences, test_sequences, train_labels, test_labels = train_test_split(
    padded_sequences, label, test_size=0.2, random_state=42)

"""## Creating the Model

For our model, we're going to use a couple of new layers to deal with our sequential data.

##Embedding Layer

This layer will take the tokenized sequences and will learn an embedding for all of the words in the training dataset. Mathematically, embeddings work the same way as a neuron in a neural network, but conceptually, their goal is to reduce the number of dimensions for some or all of the features. In this case, it will represent each word as a vector, and the information within that vector will contain the relationships between each word.

##Long Short Term Memory Layer

Our next, and very important layer, is a long short term memory layer (LSTM). An LSTM is a type of recurrent neural network or RNN. Unlike traditional feed-forward networks that we've seen so far, recurrent networks have loops in them, allowing information to persist. Here's a representation of a recurrent network:

New information (x) gets passed in to the network, which spits out a prediction (h). Additionally, information from that layer gets saved, and used as input for the next prediction. This may seem a bit complicated, but let's look at it unrolled:

We can see that when a new piece of data (x) is fed into the network, that network both spits out a prediction (h) and also passes some information along to the next layer. That next layer gets another piece of data, but gets to learn from the layer before it as well.

Traditional RNNs suffer from the issue of more recent information contributing more than information from further back. LSTMs are a special type of recurrent layer that are able to learn and retain longer term information.
Alright, let's create our model:
"""

from tensorflow.keras.layers import Embedding, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input,Reshape, Embedding, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM

import tensorflow as tf


inputs = Input(shape=(max_sequence_length,))

# Embedding layer
embedding_layer = Embedding(total_words, 100, mask_zero=True)(inputs)

# LSTM layer
lstm_layer = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)

batch_norm_layer = BatchNormalization()(lstm_layer)



dropout_layer = Dropout(0.2)(lstm_layer)

# Output layer
outputs = Dense(1, activation='sigmoid')(lstm_layer)

# Model
model = Model(inputs=inputs, outputs=outputs)

model.summary()

"""## Compiling the Model

we compile our model with binary crossentropy, as we are classifying text as offensive or non-offensive.

We are also going to select a particular optimizer that is well suited for LSTM tasks, called the *Adam* optimizer.
"""

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

"""## Training the Model

Similar to earlier sections, we fit our model in the same way. This time we'll train for 10 epochs, which will take a few minutes.
"""

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, Callback
class CustomEarlyStopping(Callback):
    def __init__(self, target_accuracy):
        super(CustomEarlyStopping, self).__init__()
        self.target_accuracy = target_accuracy

    def on_epoch_end(self, epoch, logs=None):
        if logs.get('val_accuracy') >= self.target_accuracy:
            self.model.stop_training = True
            print("Reached target accuracy of", self.target_accuracy, "Stopping training.")

optimizer = Adam(learning_rate=0.01)
early_stopping = CustomEarlyStopping(target_accuracy=0.84)


model.fit(input_sequences, train_labels, epochs=10, batch_size=64,
          validation_data=(test_sequences, test_labels),callbacks=[early_stopping],verbose=1)

"""## Discussion of Results

Essentially, it was apparent that the training accuracy increased over the course of training.However, validation accuracy kept varying between 82% and 84%.In order to attempt to resolve this issue, I tried to add a GRU layer as well as implement batch normalization,vary training parameters,vary learning rate, perform dropout regularization and cross validation, but all of these methods did not improve the validation accuracy of my model.I therfore used early stopping to stop my training process at 84% validation accuracy, which occured at epoch 2 of training.Our model therfore has a training accuracy of 93.06% and validation accuracy of 84.02%. I suspect that the validation accuracy can be increased if the code is improved to take into account the different varying nature of arabizi text and the fact that it allows us to express the same meaning but using different letters.
"""